{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "import necessary libraries"
      ],
      "metadata": {
        "id": "y_t4KTV6yCc5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "n2UFiJZpxvG_"
      },
      "outputs": [],
      "source": [
        "from imutils import build_montages\n",
        "import numpy as np\n",
        "import cv2\n",
        "import pickle\n",
        "from matplotlib import pyplot as plt\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "make pair in datasets like mnist,cifar10,imagenet,..."
      ],
      "metadata": {
        "id": "LPG1EDbVyLOR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# percentage of each technique for make same pairs\n",
        "# different pairs in mnist are same for all states\n",
        "same_pairs = [80,10,10] #[same class, rotate, noise]\n",
        "def convert_to_percentage():\n",
        "  assert sum(same_pairs) == 100\n",
        "  return (same_pairs[0],same_pairs[0]+same_pairs[1],same_pairs[0]+same_pairs[1]+same_pairs[2])\n",
        "\n",
        "def advance_make_pairs(images, labels, count):\n",
        "  pairImages = []\n",
        "  pairLabels = []\n",
        "  numClasses = len(np.unique(labels))\n",
        "  idx = [np.where(labels == i)[0] for i in range(0, numClasses)]\n",
        "  counter_1 = int(count / (2))\n",
        "  pos_counter = 0\n",
        "  neg_counter = 0\n",
        "  same_label_counter = 0\n",
        "  rotate_counter = 0\n",
        "  noise_counter = 0\n",
        "  same_percentages = convert_to_percentage()\n",
        "  for i in range(counter_1):\n",
        "    choose_augment = random.randint(1,100)\n",
        "    label = np.random.randint(0,numClasses)\n",
        "    first_img_pos = np.random.choice(idx[label])\n",
        "    first_img = images[first_img_pos]\n",
        "    if choose_augment <= same_percentages[0]:\n",
        "      second_img_pos = np.random.choice(idx[label])\n",
        "      second_img = images[second_img_pos]\n",
        "      pos_counter += 1\n",
        "      same_label_counter+=1\n",
        "    elif choose_augment <= same_percentages[1]:\n",
        "      image_center = tuple(np.array(first_img.shape[1::-1]) / 2)\n",
        "      angle = random.randint(-10,10)\n",
        "      rot_mat = cv2.getRotationMatrix2D(image_center, angle, 1.0)\n",
        "      second_img = cv2.warpAffine(first_img, rot_mat, first_img.shape[1::-1], flags=cv2.INTER_LINEAR)\n",
        "      pos_counter += 1\n",
        "      rotate_counter+=1\n",
        "    elif choose_augment <= same_percentages[2]:\n",
        "      second_img =  cv2.GaussianBlur(first_img, (3, 3), 0)\n",
        "      pos_counter += 1\n",
        "      noise_counter+=1\n",
        "      #print('noise')\n",
        "    pairImages.append([first_img, second_img])\n",
        "    pairLabels.append([1])\n",
        "  counter_2 = count - counter_1\n",
        "  i=0\n",
        "  while i < counter_2:\n",
        "    for j in range(10):\n",
        "      for z in range(j+1,10):\n",
        "        first_label = j\n",
        "        second_label = z\n",
        "        first_img_pos = np.random.choice(idx[first_label])\n",
        "        second_img_pos = np.random.choice(idx[second_label])\n",
        "        first_img = images[first_img_pos]\n",
        "        second_img = images[second_img_pos]\n",
        "        pairImages.append([first_img, second_img])\n",
        "        pairLabels.append([0])\n",
        "        neg_counter += 1\n",
        "        i+=1\n",
        "        if (i==counter_2):\n",
        "          break\n",
        "      if (i==counter_2):\n",
        "        break\n",
        "    if (i == counter_2):\n",
        "      break\n",
        "  print('**********')\n",
        "  print(f'total positive pairs:{pos_counter} equal to {(pos_counter/count)*100}')\n",
        "  print(f'total negagtive pairs:{neg_counter} equal to {(neg_counter/count)*100}')\n",
        "  print(f'total same_label :{same_label_counter} equal to {(same_label_counter/pos_counter)*100} of positives')\n",
        "  print(f'total rotate :{rotate_counter} equal to {(rotate_counter/pos_counter)*100} of positives')\n",
        "  print(f'total noise :{noise_counter} equal to {(noise_counter/pos_counter)*100} of positives')\n",
        "  print('********')\n",
        "  return (np.array(pairImages), np.array(pairLabels))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cpKgM77Hxz4g"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "make pair in our dataset(khorma)"
      ],
      "metadata": {
        "id": "jrKuP6t01KCI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#            [same class,  180 rotate,    noise,     0-flie,      1-flip]\n",
        "same_pairs = [15,60,5,10,10]\n",
        "#                 [salem kharab,salem mkharab, msalem kharab, salem msalem, msalem mkharab, mkharab kharab]\n",
        "different_pairs = [45,20,20,5,5,5]\n",
        "def convert_to_percentage_same():\n",
        "  assert sum(same_pairs) == 100\n",
        "  return (same_pairs[0],same_pairs[0]+same_pairs[1],same_pairs[0]+same_pairs[1]+same_pairs[2],same_pairs[0]+same_pairs[1]+same_pairs[2]+same_pairs[3],\n",
        "          same_pairs[0]+same_pairs[1]+same_pairs[2]+same_pairs[3]+same_pairs[4])\n",
        "def convert_to_percentage_diff():\n",
        "  assert sum(different_pairs) == 100\n",
        "  result = []\n",
        "  for i in range(len(different_pairs)):\n",
        "    result.apppend(sum(l[:i]))\n",
        "  return result\n",
        "def khorma_advance_make_pairs(images, labels, count):\n",
        "  pairImages = []\n",
        "  pairLabels = []\n",
        "  numClasses = len(np.unique(labels))\n",
        "  idx = [np.where(labels == i)[0] for i in range(0, numClasses)]\n",
        "  counter_1 = int(count / (2))\n",
        "  pos_counter = 0\n",
        "  neg_counter = 0\n",
        "  same_label_counter = 0\n",
        "  rotate_counter = 0\n",
        "  flip_counter = 0\n",
        "  noise_counter = 0\n",
        "  same_pair = convert_to_percentage_same()\n",
        "  for i in range(counter_1):\n",
        "    choose_augment = np.random.randint(1,100)\n",
        "    label = np.random.randint(0,numClasses)\n",
        "    first_img_pos = np.random.choice(idx[label])\n",
        "    first_img = images[first_img_pos]\n",
        "    if choose_augment <= same_pair[0]:\n",
        "      second_img_pos = np.random.choice(idx[label])\n",
        "      second_img = images[second_img_pos]\n",
        "      pos_counter += 1\n",
        "      same_label_counter+=1\n",
        "    elif choose_augment <= same_pair[1]:\n",
        "      second_img = cv2.rotate(first_img, cv2.ROTATE_180)\n",
        "      pos_counter += 1\n",
        "      rotate_counter+=1\n",
        "      #print('rotate')\n",
        "    elif choose_augment <= same_pair[2]:\n",
        "      second_img =  cv2.GaussianBlur(first_img, (7, 7), 0)\n",
        "      pos_counter += 1\n",
        "      noise_counter+=1\n",
        "      #print('noise')\n",
        "    elif choose_augment <=same_pair[3]:\n",
        "      second_img = cv2.flip(first_img, 0)\n",
        "      pos_counter += 1\n",
        "      flip_counter+=1\n",
        "      #print('flip')\n",
        "    elif choose_augment<=same_pair[4]:\n",
        "      second_img = cv2.flip(first_img, 1)\n",
        "      pos_counter += 1\n",
        "      flip_counter += 1\n",
        "    pairImages.append([first_img, second_img])\n",
        "    pairLabels.append([1])\n",
        "  counter_2 = count - counter_1\n",
        "  i=0\n",
        "  dif_pairs = convert_to_percentage_diff()\n",
        "  while i < counter_2:\n",
        "    choose_neg_pair = np.random.randint(1,100)\n",
        "    if choose_neg_pair <= dif_pairs[0]:\n",
        "      first_label = 0\n",
        "      second_label =3\n",
        "    elif choose_neg_pair <= dif_pairs[1]:\n",
        "      first_label = 0\n",
        "      second_label = 2\n",
        "    elif choose_neg_pair <= dif_pairs[2]:\n",
        "      first_label = 1\n",
        "      second_label = 3\n",
        "    elif choose_neg_pair <= dif_pairs[3]:\n",
        "      first_label = 0\n",
        "      second_label = 1\n",
        "    elif choose_neg_pair <= dif_pairs[4]:\n",
        "      first_label = 1\n",
        "      second_label = 2\n",
        "    elif choose_neg_pair <= dif_pairs[5]:\n",
        "      first_label = 2\n",
        "      second_label = 3\n",
        "    first_img_pos = np.random.choice(idx[first_label])\n",
        "    second_img_pos = np.random.choice(idx[second_label])\n",
        "    first_img = images[first_img_pos]\n",
        "    second_img = images[second_img_pos]\n",
        "    pairImages.append([first_img, second_img])\n",
        "    pairLabels.append([0])\n",
        "    neg_counter += 1\n",
        "    i+=1\n",
        "    if (i==counter_2):\n",
        "      break\n",
        "  print('**********')\n",
        "  print(f'total positive pairs:{pos_counter} equal to {(pos_counter/count)*100}')\n",
        "  print(f'total negagtive pairs:{neg_counter} equal to {(neg_counter/count)*100}')\n",
        "  print(f'total same_label :{same_label_counter} equal to {(same_label_counter/pos_counter)*100} of positives')\n",
        "  print(f'total rotate :{rotate_counter} equal to {(rotate_counter/pos_counter)*100} of positives')\n",
        "  print(f'total flip :{flip_counter} equal to {(flip_counter/pos_counter)*100} of positives')\n",
        "  print(f'total noise :{noise_counter} equal to {(noise_counter/pos_counter)*100} of positives')\n",
        "  print('********')\n",
        "  return (np.array(pairImages), np.array(pairLabels))"
      ],
      "metadata": {
        "id": "eUuqxx7Z1FaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "load dataset"
      ],
      "metadata": {
        "id": "dZ5YjvLOGMEP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow\n",
        "mnist = tensorflow.keras.datasets.mnist\n",
        "(x_train,y_train), (x_valid, y_valid) = mnist.load_data()"
      ],
      "metadata": {
        "id": "INNLYIv8GLp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "configs"
      ],
      "metadata": {
        "id": "4z9ZL57PGVnA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "IMG_SHAPE = (28, 28,1)\n",
        "BATCH_SIZE = 20\n",
        "EPOCHS = 10\n",
        "BASE_OUTPUT = \"output\"\n",
        "MODEL_PATH = os.path.sep.join([BASE_OUTPUT, \"siamese_model\"])\n",
        "PLOT_PATH = os.path.sep.join([BASE_OUTPUT, \"plot.png\"])"
      ],
      "metadata": {
        "id": "ZEuyLEokGPXf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "define your model here"
      ],
      "metadata": {
        "id": "fFT53mQkGXjf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import neccessary packages\n",
        "from tensorflow.keras.models import Model,Sequential\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Conv2D, Flatten, BatchNormalization\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "import tensorflow\n",
        "\n",
        "def build_siamese_model(inputShape, embeddingDim=70):\n",
        "  inputs = Input(inputShape)\n",
        "  x = Conv2D(64, (7, 7), activation='relu', padding='valid')(inputs)\n",
        "\n",
        "  x = Conv2D(64, (5, 5), activation='relu', padding='valid')(x)\n",
        "\n",
        "  x = Conv2D(128, (5, 5), activation='relu', padding='valid')(x)\n",
        "\n",
        "  x = Conv2D(128, (5, 5), activation='relu', padding='valid')(x)\n",
        "\n",
        "  x = Conv2D(128, (5, 5), activation='relu', padding='valid')(x)\n",
        "\n",
        "  x = Conv2D(128, (3, 3), activation='relu', padding='valid')(x)\n",
        "\n",
        "  x = Conv2D(256, (3, 3), activation='relu', padding='valid')(x)\n",
        "\n",
        "  pooledOutput = GlobalAveragePooling2D()(x)\n",
        "  outputs = Dense(embeddingDim)(pooledOutput)\n",
        "  model = Model(inputs, outputs)\n",
        "\t# return the model to the calling function\n",
        "  model.summary()\n",
        "  return model\n"
      ],
      "metadata": {
        "id": "YUvxLPvkGXQf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "utils"
      ],
      "metadata": {
        "id": "O9JZRykNGhNZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.keras.backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "def euclidean_distance(vectors):\n",
        "\t# unpack the vectors into separate lists\n",
        "\t(featsA, featsB) = vectors\n",
        "\t# compute the sum of squared distances between the vectors\n",
        "\tsumSquared = K.sum(K.square(featsA - featsB), axis=1,\n",
        "\t\tkeepdims=True)\n",
        "\t# return the euclidean distance between the vectors\n",
        "\treturn K.sqrt(K.maximum(sumSquared, K.epsilon()))\n",
        "\n",
        "def plot_training(H, plotPath):\n",
        "\t# construct a plot that plots and saves the training history\n",
        "\tplt.style.use(\"ggplot\")\n",
        "\tplt.figure()\n",
        "\tplt.plot(H.history[\"loss\"], label=\"train_loss\")\n",
        "\tplt.plot(H.history[\"val_loss\"], label=\"val_loss\")\n",
        "\tplt.plot(H.history[\"accuracy\"], label=\"train_acc\")\n",
        "\tplt.plot(H.history[\"val_accuracy\"], label=\"val_acc\")\n",
        "\tplt.title(\"Training Loss and Accuracy\")\n",
        "\tplt.xlabel(\"Epoch #\")\n",
        "\tplt.ylabel(\"Loss/Accuracy\")\n",
        "\tplt.legend(loc=\"lower left\")\n",
        "\tplt.savefig(plotPath)"
      ],
      "metadata": {
        "id": "jg9QU1WtGiAv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "create dataset"
      ],
      "metadata": {
        "id": "8tFC4niaGuZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import packages\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Lambda\n",
        "import numpy as np\n",
        "import cv2\n",
        "train_size = 5000 # define number of train data\n",
        "test_size = 100 # define number of test data\n",
        "x_train = x_train / 255.0\n",
        "x_valid = x_valid / 255.0\n",
        "(pairTrain, labelTrain) = advance_make_pairs(x_train, y_train, train_size)\n",
        "(pairTest, labelTest) = advance_make_pairs(x_train,y_train, test_size)\n"
      ],
      "metadata": {
        "id": "T--G8-L2Gi1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRuSdMUjGytL",
        "outputId": "7b3fc7a7-45f2-4ad2-ba1b-96b52ac04019"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "fit model with dataset"
      ],
      "metadata": {
        "id": "Psjz7d4HG1TI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.api._v2.keras import callbacks\n",
        "import keras\n",
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.3,\n",
        "                                              patience=5, min_lr=1e-10)\n",
        "\n",
        "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
        "imgA = Input(shape=IMG_SHAPE)\n",
        "imgB = Input(shape=IMG_SHAPE)\n",
        "featureExtractor = build_siamese_model(IMG_SHAPE,70)\n",
        "featsA = featureExtractor(imgA)\n",
        "featsB = featureExtractor(imgB)\n",
        "distance = Lambda(euclidean_distance)([featsA, featsB])\n",
        "outputs = Dense(1, activation=\"sigmoid\")(distance)\n",
        "model = Model(inputs=[imgA, imgB], outputs=outputs)\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",\n",
        "\tmetrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "history = model.fit(\n",
        "\t[pairTrain[:, 0], pairTrain[:, 1]], labelTrain[:],\n",
        "\tvalidation_data=([pairTest[:, 0], pairTest[:, 1]], labelTest[:]),\n",
        "\tbatch_size=BATCH_SIZE,\n",
        "\tepochs=50,\n",
        "\tcallbacks = [reduce_lr,early_stopping])\n",
        "model.save(MODEL_PATH)\n",
        "plot_training(history, PLOT_PATH)\n",
        "model.save_weights('./siamese_network.h5') # you can define your specific path here"
      ],
      "metadata": {
        "id": "HCpdkVAoGzXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p=model.predict([pairTest[:, 0], pairTest[:, 1]])"
      ],
      "metadata": {
        "id": "BwlwBGvnI4cJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "store train and validation features for more executaion speed"
      ],
      "metadata": {
        "id": "NtQ5wrcEHEa4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_featured = model.layers[2].predict(x_train)\n",
        "valid_featured = model.layers[2].predict(x_valid)\n",
        "np.save('./train_features.npy', train_featured) # define your path here\n",
        "np.save('./valid_features.npy', valid_featured) # define your path here"
      ],
      "metadata": {
        "id": "a0W6Pd44HD4p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}