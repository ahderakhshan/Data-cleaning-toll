{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpdzfGZerGfy"
      },
      "source": [
        "import necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4G4fzHAxq6ak"
      },
      "outputs": [],
      "source": [
        "from imutils import build_montages\n",
        "import numpy as np\n",
        "import cv2\n",
        "import pickle\n",
        "from matplotlib import pyplot as plt\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Snll86M6ruaU"
      },
      "source": [
        "load our dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GUxu-LePrtkR"
      },
      "outputs": [],
      "source": [
        "import tensorflow\n",
        "mnist = tensorflow.keras.datasets.mnist\n",
        "(x_train,y_train), (x_valid, y_valid) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4OInw38sOgf"
      },
      "source": [
        "configs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YunheiL2sOMC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "IMG_SHAPE = (28, 28,1)\n",
        "BATCH_SIZE = 20\n",
        "EPOCHS = 10\n",
        "BASE_OUTPUT = \"output\"\n",
        "MODEL_PATH = os.path.sep.join([BASE_OUTPUT, \"siamese_model\"])\n",
        "PLOT_PATH = os.path.sep.join([BASE_OUTPUT, \"plot.png\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ginfE-7JsRPE"
      },
      "source": [
        "create simple network it depends on our siamese model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBCNeT0wsQ8z"
      },
      "outputs": [],
      "source": [
        "# import neccessary packages\n",
        "from tensorflow.keras.models import Model,Sequential\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Conv2D, Flatten, BatchNormalization\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "import tensorflow\n",
        "\n",
        "def build_siamese_model(inputShape, embeddingDim=70):\n",
        "  inputs = Input(inputShape)\n",
        "  x = Conv2D(64, (7, 7), activation='relu', padding='valid')(inputs)\n",
        "\n",
        "  x = Conv2D(64, (5, 5), activation='relu', padding='valid')(x)\n",
        "\n",
        "  x = Conv2D(128, (5, 5), activation='relu', padding='valid')(x)\n",
        "\n",
        "  x = Conv2D(128, (5, 5), activation='relu', padding='valid')(x)\n",
        "\n",
        "  x = Conv2D(128, (5, 5), activation='relu', padding='valid')(x)\n",
        "\n",
        "  x = Conv2D(128, (3, 3), activation='relu', padding='valid')(x)\n",
        "\n",
        "  x = Conv2D(256, (3, 3), activation='relu', padding='valid')(x)\n",
        "\n",
        "  pooledOutput = GlobalAveragePooling2D()(x)\n",
        "  outputs = Dense(embeddingDim)(pooledOutput)\n",
        "  model = Model(inputs, outputs)\n",
        "\t# return the model to the calling function\n",
        "  model.summary()\n",
        "  return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5s6MzaHtNyn"
      },
      "source": [
        "utils file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nP9U9yItPBY"
      },
      "outputs": [],
      "source": [
        "import tensorflow.keras.backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "def euclidean_distance(vectors):\n",
        "\t# unpack the vectors into separate lists\n",
        "\t(featsA, featsB) = vectors\n",
        "\t# compute the sum of squared distances between the vectors\n",
        "\tsumSquared = K.sum(K.square(featsA - featsB), axis=1,\n",
        "\t\tkeepdims=True)\n",
        "\t# return the euclidean distance between the vectors\n",
        "\treturn K.sqrt(K.maximum(sumSquared, K.epsilon()))\n",
        "\n",
        "def plot_training(H, plotPath):\n",
        "\t# construct a plot that plots and saves the training history\n",
        "\tplt.style.use(\"ggplot\")\n",
        "\tplt.figure()\n",
        "\tplt.plot(H.history[\"loss\"], label=\"train_loss\")\n",
        "\tplt.plot(H.history[\"val_loss\"], label=\"val_loss\")\n",
        "\tplt.plot(H.history[\"accuracy\"], label=\"train_acc\")\n",
        "\tplt.plot(H.history[\"val_accuracy\"], label=\"val_acc\")\n",
        "\tplt.title(\"Training Loss and Accuracy\")\n",
        "\tplt.xlabel(\"Epoch #\")\n",
        "\tplt.ylabel(\"Loss/Accuracy\")\n",
        "\tplt.legend(loc=\"lower left\")\n",
        "\tplt.savefig(plotPath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klhnUh7iuPSH"
      },
      "source": [
        "load model weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KnW4q9ljuPHI"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Lambda\n",
        "from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "imgA = Input(shape=IMG_SHAPE)\n",
        "imgB = Input(shape=IMG_SHAPE)\n",
        "featureExtractor = build_siamese_model(IMG_SHAPE)\n",
        "featsA = featureExtractor(imgA)\n",
        "featsB = featureExtractor(imgB)\n",
        "distance = Lambda(euclidean_distance)([featsA, featsB])\n",
        "outputs = Dense(1, activation=\"sigmoid\")(distance)\n",
        "model = Model(inputs=[imgA, imgB], outputs=outputs)\n",
        "model.load_weights('./siamese_network.h5') # base on where siamese network saved"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Jf_WWNjafes"
      },
      "source": [
        "load features of mnist images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "in4nb1BQA22l"
      },
      "outputs": [],
      "source": [
        "train_featured = np.load('./train_features.npy')\n",
        "valid_featured = np.load('./valid_features.npy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEjjsaOBajVP"
      },
      "source": [
        "model that predcit similarity based on features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvHGF3VESE6a"
      },
      "outputs": [],
      "source": [
        "imgA = Input(shape=(70,))\n",
        "imgB = Input(shape=(70,))\n",
        "distance = Lambda(euclidean_distance)([imgA, imgB])\n",
        "outputs = model.layers[-1](distance)\n",
        "new_model = Model(inputs=[imgA, imgB], outputs=outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SefsQCXLap81"
      },
      "source": [
        "if you have deleted some train data also delete it from train features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxSj1qhjCPvF"
      },
      "outputs": [],
      "source": [
        "index_to_delete = []\n",
        "\n",
        "index_to_delete.sort(reverse=True)\n",
        "train_featured = np.delete(train_featured,index_to_delete,axis=0)\n",
        "y_train = np.delete(y_train,index_to_delete,axis=0)\n",
        "x_train = np.delete(x_train,index_to_delete,axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zs_2k9ROucse"
      },
      "source": [
        "get similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCxRwnQYa7Yk"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "from imutils.paths import list_images\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import argparse\n",
        "import cv2\n",
        "\n",
        "NUMBER_OF_SIMILARS = train_featured.shape[0]\n",
        "# no error\n",
        "WRONG_PREDICTED_VALIDS = [321, 326, 447, 449, 582, 619, 625, 646, 659, 674, 720, 726, 947, 1014, 1114, 1232, 1242, 1260, 1299, 1344, 1459, 1737, 1901, 1903, 2033, 2035, 2109, 2129, 2130, 2135, 2225, 2454, 2462, 2488, 2597, 2654, 2939, 2953, 3225, 3422, 3558, 3762, 3985, 4065, 4078, 4176, 4289, 4497, 4504, 4536, 4699, 4761, 4783, 4823, 4860, 5165, 5937, 5955, 6101, 6173, 6400, 6560, 6571, 6576, 6597, 6651, 8094, 8287, 8527, 9123, 9638, 9642, 9664, 9692, 9693, 9729, 9811, 9839]\n",
        "def get_max_similars_to_img(img,n):\n",
        "  valid_images = []\n",
        "  no_images = train_featured.shape[0]\n",
        "  mypair1 = np.ndarray(shape=(no_images, 70))\n",
        "  mypair1[:] = valid_featured[img]\n",
        "  mypair2 = train_featured[0:no_images]\n",
        "  pred = new_model.predict([mypair2,mypair1], verbose=False)\n",
        "  pred = pred.reshape(-1)\n",
        "  ind = np.argsort(pred)[::-1][0:n]\n",
        "  similarities = pred[ind]\n",
        "  return (ind,similarities)\n",
        "def matrix_of_similars():\n",
        "  result = []\n",
        "  for img in WRONG_PREDICTED_VALIDS:\n",
        "    result.append(get_max_similars_to_img(img,NUMBER_OF_SIMILARS))\n",
        "  return result\n",
        "similars_matrix = matrix_of_similars()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8LF7ZD7cpZK"
      },
      "outputs": [],
      "source": [
        "#PARAMS\n",
        "NUM_CLASSES = 10\n",
        "CONFIDENT_PERCENTAGE = 0.9\n",
        "MIN_REPEAT = 1\n",
        "SIMILARS = 500\n",
        "def clean_data():\n",
        "  predicted_wrongs = {}\n",
        "  counter = 0\n",
        "  for row in similars_matrix:\n",
        "    indexes = row[0][:SIMILARS]\n",
        "    similarities = row[1]\n",
        "    for index in indexes:\n",
        "      try:\n",
        "        predicted_wrongs[index][y_valid[WRONG_PREDICTED_VALIDS[counter]]] += 1\n",
        "      except:\n",
        "         predicted_wrongs[index] = [0 for i in range(NUM_CLASSES)]\n",
        "         predicted_wrongs[index][y_valid[WRONG_PREDICTED_VALIDS[counter]]] += 1\n",
        "    counter += 1\n",
        "  new_predicted_wrongs = {}\n",
        "  for k,v in predicted_wrongs.items():\n",
        "    if sum(v) >= MIN_REPEAT:\n",
        "      new_predicted_wrongs[k] = [i/sum(v) for i in v]\n",
        "  for k,v in new_predicted_wrongs.items():\n",
        "    new_predicted_wrongs[k] = [max(v), v.index(max(v))]\n",
        "  new_predicted_wrongs = dict(sorted(new_predicted_wrongs.items(), key=lambda item: item[1][0], reverse=True))\n",
        "  final_result = []\n",
        "  for k,v in new_predicted_wrongs.items():\n",
        "    if v[0]> CONFIDENT_PERCENTAGE and y_train[k] != v[1]:\n",
        "      final_result.append(k)\n",
        "  print(f'find {len(final_result)} items')\n",
        "  print(final_result)\n",
        "  return final_result\n",
        "results = clean_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TV5LRmGrnvNZ"
      },
      "source": [
        "plot top 10 detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmNY6VDBuhrT"
      },
      "outputs": [],
      "source": [
        "counter = 0\n",
        "indexes = results[:10]\n",
        "print(indexes)\n",
        "import matplotlib.pyplot as plt\n",
        "def plot_images(indexes,title=None):\n",
        "    images = x_train[indexes]\n",
        "    labels = y_train[indexes]\n",
        "    fig,axs = plt.subplots(nrows=2 ,ncols=5)\n",
        "    fig.suptitle(f'top 10 detection {title}')\n",
        "    counter = 0\n",
        "   # fig.set_title('top 10 clean lab')\n",
        "    for i in range(2):\n",
        "        for j in range(5):\n",
        "            axs[i][j].imshow(images[counter])\n",
        "            axs[i][j].axis('off')\n",
        "            axs[i][j].set_title((labels[counter]))\n",
        "            counter+=1\n",
        "        #axs[i].title(labels[i])\n",
        "    plt.show()\n",
        "plot_images(indexes,f'our solution')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}